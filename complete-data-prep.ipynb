{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use some of this, but remember to keep the data organized by pair, so that I can weigh the different language pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_file(path):\n",
    "    data = []\n",
    "    with open(path, 'r') as f:\n",
    "        for line in f:\n",
    "            data.append(line)\n",
    "    return data\n",
    "\n",
    "def combine_src_and_tgt(src, tgt):\n",
    "    combined = []\n",
    "    for i in range(len(src)):\n",
    "        sentences = (src[i], tgt[i])\n",
    "        combined.append(sentences)\n",
    "    return combined\n",
    "\n",
    "def get_random_sentences_from_file(path, n=-1):\n",
    "    '''\n",
    "    Finds the src and tgt file from the path and returns n sentences \n",
    "    from the files as a list of tuples with the src and tgt sentences.\n",
    "    '''\n",
    "\n",
    "    src_path = path + '.src.sp'\n",
    "    tgt_path = path + '.tgt.sp'\n",
    "\n",
    "    data = combine_src_and_tgt(open_file(src_path), open_file(tgt_path))\n",
    "\n",
    "    if n == -1:\n",
    "        sampled_indices = random.sample(range(len(data)), len(data))\n",
    "    else:\n",
    "        sampled_indices = random.sample(range(len(data)), n)\n",
    "\n",
    "    sampled_data = [data[i] for i in sampled_indices]\n",
    "\n",
    "    return sampled_data\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add all the data to the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_test_set = []\n",
    "complete_test_set += get_random_sentences_from_file('cleaned-tokenized-tagged/english/km-en-test', 500)\n",
    "complete_test_set += get_random_sentences_from_file('cleaned-tokenized-tagged/english/lo-en-test', 500)\n",
    "complete_test_set += get_random_sentences_from_file('cleaned-tokenized-tagged/english/th-en-test', 500)\n",
    "complete_test_set += get_random_sentences_from_file('cleaned-tokenized-tagged/khmer/en-km-test', 500)\n",
    "complete_test_set += get_random_sentences_from_file('cleaned-tokenized-tagged/khmer/lo-km-test', 500)\n",
    "complete_test_set += get_random_sentences_from_file('cleaned-tokenized-tagged/khmer/th-km-test', 500)\n",
    "complete_test_set += get_random_sentences_from_file('cleaned-tokenized-tagged/lao/en-lo-test', 500)\n",
    "complete_test_set += get_random_sentences_from_file('cleaned-tokenized-tagged/lao/km-lo-test', 500)\n",
    "complete_test_set += get_random_sentences_from_file('cleaned-tokenized-tagged/lao/th-lo-test', 500)\n",
    "complete_test_set += get_random_sentences_from_file('cleaned-tokenized-tagged/thai/en-th-test', 500)\n",
    "complete_test_set += get_random_sentences_from_file('cleaned-tokenized-tagged/thai/lo-th-test', 500)\n",
    "complete_test_set += get_random_sentences_from_file('cleaned-tokenized-tagged/thai/km-th-test', 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add all the data to the validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_val_set = []\n",
    "complete_val_set += get_random_sentences_from_file('cleaned-tokenized-tagged/english/km-en-val', 500)\n",
    "complete_val_set += get_random_sentences_from_file('cleaned-tokenized-tagged/english/lo-en-val', 500)\n",
    "complete_val_set += get_random_sentences_from_file('cleaned-tokenized-tagged/english/th-en-val', 500)\n",
    "complete_val_set += get_random_sentences_from_file('cleaned-tokenized-tagged/khmer/en-km-val', 500)\n",
    "complete_val_set += get_random_sentences_from_file('cleaned-tokenized-tagged/khmer/lo-km-val', 500)\n",
    "complete_val_set += get_random_sentences_from_file('cleaned-tokenized-tagged/khmer/th-km-val', 500)\n",
    "complete_val_set += get_random_sentences_from_file('cleaned-tokenized-tagged/lao/en-lo-val', 500)\n",
    "complete_val_set += get_random_sentences_from_file('cleaned-tokenized-tagged/lao/km-lo-val', 500)\n",
    "complete_val_set += get_random_sentences_from_file('cleaned-tokenized-tagged/lao/th-lo-val', 500)\n",
    "complete_val_set += get_random_sentences_from_file('cleaned-tokenized-tagged/thai/en-th-val', 500)\n",
    "complete_val_set += get_random_sentences_from_file('cleaned-tokenized-tagged/thai/lo-th-val', 500)\n",
    "complete_val_set += get_random_sentences_from_file('cleaned-tokenized-tagged/thai/km-th-val', 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shuffle the data in both sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(complete_test_set)\n",
    "random.shuffle(complete_val_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write both data sets to files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_file(path, data):\n",
    "    with open(path, 'w') as f:\n",
    "        for s in data:\n",
    "            f.write(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_src = [pair[0] for pair in complete_test_set]\n",
    "test_tgt = [pair[1] for pair in complete_test_set]\n",
    "\n",
    "val_src = [pair[0] for pair in complete_val_set]\n",
    "val_tgt = [pair[1] for pair in complete_val_set]\n",
    "\n",
    "test_src_path = 'complete/data/tokenized/src-test.txt'\n",
    "test_tgt_path = 'complete/data/tokenized/tgt-test.txt'\n",
    "val_src_path = 'complete/data/tokenized/src-val.txt'\n",
    "val_tgt_path = 'complete/data/tokenized/tgt-val.txt'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_to_file(test_src_path, test_src)\n",
    "write_to_file(test_tgt_path, test_tgt)\n",
    "write_to_file(val_src_path, val_src)\n",
    "write_to_file(val_tgt_path, val_tgt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do this for the training data too!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_train_set = []\n",
    "complete_train_set += get_random_sentences_from_file('cleaned-tagged/english/km-en-train')\n",
    "complete_train_set += get_random_sentences_from_file('cleaned-tagged/english/lo-en-train')\n",
    "complete_train_set += get_random_sentences_from_file('cleaned-tagged/english/th-en-train')\n",
    "complete_train_set += get_random_sentences_from_file('cleaned-tagged/khmer/en-km-train')\n",
    "complete_train_set += get_random_sentences_from_file('cleaned-tagged/khmer/lo-km-train')\n",
    "complete_train_set += get_random_sentences_from_file('cleaned-tagged/khmer/th-km-train')\n",
    "complete_train_set += get_random_sentences_from_file('cleaned-tagged/lao/en-lo-train')\n",
    "complete_train_set += get_random_sentences_from_file('cleaned-tagged/lao/km-lo-train')\n",
    "complete_train_set += get_random_sentences_from_file('cleaned-tagged/lao/th-lo-train')\n",
    "complete_train_set += get_random_sentences_from_file('cleaned-tagged/thai/en-th-train')\n",
    "complete_train_set += get_random_sentences_from_file('cleaned-tagged/thai/lo-th-train')\n",
    "complete_train_set += get_random_sentences_from_file('cleaned-tagged/thai/km-th-train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4138376\n"
     ]
    }
   ],
   "source": [
    "print(len(complete_train_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(complete_train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_src = [pair[0] for pair in complete_train_set]\n",
    "train_tgt = [pair[1] for pair in complete_train_set]\n",
    "\n",
    "train_src_path = 'complete/data/src-train.txt'\n",
    "train_tgt_path = 'complete/data/tgt-train.txt'\n",
    "\n",
    "write_to_file(train_src_path, train_src)\n",
    "write_to_file(train_tgt_path, train_tgt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
